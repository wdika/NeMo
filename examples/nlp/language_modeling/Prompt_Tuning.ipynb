{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c4e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.plugins.precision.native_amp import NativeMixedPrecisionPlugin\n",
    "from pytorch_lightning.trainer.connectors.checkpoint_connector import CheckpointConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f035deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "from nemo.collections.nlp.modules.common.megatron.megatron_utils import compute_model_parallel_rank\n",
    "from nemo.collections.nlp.parts.nlp_overrides import GradScaler, NLPDDPPlugin, NLPSaveRestoreConnector\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.exp_manager import StatelessTimer, exp_manager\n",
    "from nemo.utils.config_utils import update_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f407d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"conf/megatron_gpt_config.yaml\")\n",
    "\n",
    "cfg.trainer.gpus = 1\n",
    "\n",
    "# Set current model params\n",
    "cfg.model.encoder_seq_length = 2048\n",
    "\n",
    "# Set prompt tuning params\n",
    "cfg.model.optim.lr = 2e-4\n",
    "cfg.model.optim.sched.min_lr = 2e-6\n",
    "cfg.model.use_soft_prompts = True\n",
    "cfg.model.prompt_length = 10\n",
    "cfg.model.data.train_ds = 'prompt_tuning_ner_train.json'\n",
    "cfg.model.data.valid_ds = 'prompt_tuning_ner_val.json'\n",
    "cfg.model.data.test_ds = 'prompt_tuning_ner_test.json'\n",
    "cfg.model.data.batch_size = 32\n",
    "cfg.model.data.data_prefix = None\n",
    "cfg.model.optim.sched.warmup_steps = 100\n",
    "cfg.model.optim.sched.constant_steps = 1000\n",
    "cfg.trainer.max_steps = 3000\n",
    "cfg.restore_from_path = 'megatron_gpt.nemo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5172227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:107: LightningDeprecationWarning: Argument `num_nodes` in `DDPPlugin` is deprecated in v1.4, and will be removed in v1.6. Notice that it will be overriden by the trainer setting.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:113: LightningDeprecationWarning: Argument `sync_batchnorm` in `DDPPlugin` is deprecated in v1.4, and will be removed in v1.6. Notice that it will be overriden by the trainer setting.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:324: LightningDeprecationWarning: Passing <nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7fab608a4f10> `strategy` to the `plugins` flag in Trainer has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy=<nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7fab608a4f10>)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2021-12-09 08:24:49 exp_manager:558] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2021-12-09 08:24:49 exp_manager:411] There was no checkpoint folder at checkpoint_dir :/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:24:49 exp_manager:283] Experiments will be logged at /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt\n",
      "[NeMo I 2021-12-09 08:24:49 exp_manager:648] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-09 08:24:49 exp_manager:879] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2021-12-09 08:24:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:24:52 tokenizer_utils:188] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m and custom vocab file: None\n",
      "[NeMo I 2021-12-09 08:24:52 tokenizer_utils:124] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:24:56 megatron_gpt_model:531] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2021-12-09 08:24:57 tokenizer_utils:188] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m and custom vocab file: None\n",
      "[NeMo I 2021-12-09 08:24:57 tokenizer_utils:124] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:25:01 megatron_gpt_model:531] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2021-12-09 08:25:01 save_restore_connector:149] Model MegatronGPTModel was successfully restored from /prompt-tuning/NeMo/examples/nlp/language_modeling/megatron_gpt.nemo.\n"
     ]
    }
   ],
   "source": [
    "plugins = [NLPDDPPlugin(num_nodes=cfg.trainer.num_nodes)]\n",
    "\n",
    "if cfg.get('cluster_type', None) == 'BCP':\n",
    "    plugins.append(TorchElasticEnvironment())\n",
    "\n",
    "trainer = Trainer(plugins=plugins, **cfg.trainer)\n",
    "\n",
    "exp_manager(trainer, cfg.exp_manager)\n",
    "\n",
    "model = MegatronGPTModel.restore_from(cfg.restore_from_path, cfg.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e990c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prompt_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_prompt_from_text(\"NER-Yes-No\", \"named entities yes or no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba1b3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NER-Yes-No'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prompt_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4e8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_prompt_from_text(\"NER-Complete\", \"name chemicals entities in context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103cf959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NER-Complete', 'NER-Yes-No'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prompt_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prompt_tuning_freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f91cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0025, -0.0637,  0.0431,  ..., -0.0647, -0.0375,  0.0141],\n",
      "        [ 0.0106, -0.0536,  0.0301,  ..., -0.0778, -0.0132, -0.0020],\n",
      "        [-0.0194, -0.0314, -0.0012,  ..., -0.0174, -0.0120, -0.0127],\n",
      "        ...,\n",
      "        [-0.0194, -0.0314, -0.0012,  ..., -0.0174, -0.0120, -0.0127],\n",
      "        [-0.0097, -0.0083,  0.0061,  ...,  0.0364,  0.0232, -0.0137],\n",
      "        [ 0.0054,  0.0301, -0.0031,  ...,  0.0367, -0.0045,  0.0023]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031,  0.0119,  0.0349,  ...,  0.0126,  0.0047, -0.0095],\n",
      "        [ 0.0109,  0.0207,  0.0084,  ...,  0.0139,  0.0015, -0.0072],\n",
      "        [-0.0033, -0.0169, -0.0171,  ...,  0.0173,  0.0033, -0.0068],\n",
      "        ...,\n",
      "        [-0.0072, -0.0287, -0.0161,  ...,  0.0002,  0.0034, -0.0015],\n",
      "        [ 0.0021, -0.0283, -0.0269,  ..., -0.0048,  0.0075, -0.0037],\n",
      "        [-0.0031, -0.0270, -0.0164,  ...,  0.0128,  0.0079, -0.0024]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0060, -0.0364,  0.0361,  ..., -0.0488, -0.0359,  0.0109],\n",
      "        [-0.0197, -0.0674, -0.0049,  ..., -0.0641, -0.0196,  0.0043],\n",
      "        [ 0.0106, -0.0536,  0.0301,  ..., -0.0778, -0.0132, -0.0020],\n",
      "        ...,\n",
      "        [ 0.0106, -0.0536,  0.0301,  ..., -0.0778, -0.0132, -0.0020],\n",
      "        [-0.0158,  0.0486, -0.0008,  ...,  0.0577, -0.0127,  0.0062],\n",
      "        [-0.0045, -0.0382,  0.0003,  ..., -0.0254, -0.0256, -0.0168]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031,  0.0119,  0.0349,  ...,  0.0126,  0.0047, -0.0095],\n",
      "        [ 0.0109,  0.0207,  0.0084,  ...,  0.0139,  0.0015, -0.0072],\n",
      "        [-0.0033, -0.0169, -0.0171,  ...,  0.0173,  0.0033, -0.0068],\n",
      "        ...,\n",
      "        [-0.0072, -0.0287, -0.0161,  ...,  0.0002,  0.0034, -0.0015],\n",
      "        [ 0.0021, -0.0283, -0.0269,  ..., -0.0048,  0.0075, -0.0037],\n",
      "        [-0.0031, -0.0270, -0.0164,  ...,  0.0128,  0.0079, -0.0024]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a4fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-09 08:25:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:25:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "I1209 08:25:01.515900 140379858880320 distributed_c10d.py:218] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "I1209 08:25:01.517103 140379858880320 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "I1209 08:25:01.520228 140379858880320 distributed_c10d.py:218] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "I1209 08:25:01.521262 140379858880320 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "I1209 08:25:01.522430 140379858880320 distributed_c10d.py:218] Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "I1209 08:25:01.523324 140379858880320 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "I1209 08:25:01.524626 140379858880320 distributed_c10d.py:218] Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "I1209 08:25:01.525450 140379858880320 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "I1209 08:25:01.526950 140379858880320 distributed_c10d.py:218] Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "I1209 08:25:01.528177 140379858880320 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "I1209 08:25:01.529425 140379858880320 distributed_c10d.py:218] Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "I1209 08:25:01.530461 140379858880320 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> initializing data parallel with size 1\n",
      "[NeMo I 2021-12-09 08:25:01 nlp_overrides:118] mp_rank: 0\n",
      "[NeMo I 2021-12-09 08:25:01 nlp_overrides:119] dp_rank: 0\n",
      "[NeMo I 2021-12-09 08:25:01 gpt_prompt_tuning_dataset:57] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17341it [00:06, 2488.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:25:08 gpt_prompt_tuning_dataset:78] Skipped 9 sentences, sequence length too long or too short\n",
      "[NeMo I 2021-12-09 08:25:08 gpt_prompt_tuning_dataset:57] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4141it [00:01, 2595.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:25:10 gpt_prompt_tuning_dataset:78] Skipped 2 sentences, sequence length too long or too short\n",
      "[NeMo I 2021-12-09 08:25:10 gpt_prompt_tuning_dataset:57] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10190it [00:03, 2793.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:25:13 gpt_prompt_tuning_dataset:78] Skipped 22 sentences, sequence length too long or too short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:25:13 modelPT:561] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2021-12-09 08:25:13 lr_scheduler:748] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7faaee41b580>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 100\n",
      "    constant_steps: 600\n",
      "    min_lr: 2.0e-06\n",
      "    max_steps: 1000\n",
      "    )\n",
      "[NeMo I 2021-12-09 08:25:13 nlp_overrides:80] Configuring DDP for model parallelism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | GPTModel | 125 M \n",
      "-----------------------------------\n",
      "30.7 K    Trainable params\n",
      "125 M     Non-trainable params\n",
      "125 M     Total params\n",
      "250.586   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-09 08:25:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:25:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:220: UserWarning: You called `self.log('consumed_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:25:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8bd25ca7b0417ab358af9f97a9290b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-12-09 08:25:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:220: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2021-12-09 08:25:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:220: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "I1209 08:25:15.816919 140379858880320 distributed.py:872] Reducer buckets have been rebuilt in this iteration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 99: val_loss reached 3.24958 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=3.25-step=99-consumed_samples=396.0.ckpt\" as top 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 199: val_loss reached 6.03490 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=6.03-step=199-consumed_samples=796.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:25:57 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=3.25-step=99-consumed_samples=396.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 299: val_loss reached 5.96162 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.96-step=299-consumed_samples=1196.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:26:20 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=6.03-step=199-consumed_samples=796.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 399: val_loss reached 5.91219 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.91-step=399-consumed_samples=1596.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:26:43 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.96-step=299-consumed_samples=1196.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 499: val_loss reached 5.90947 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.91-step=499-consumed_samples=1996.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:27:07 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.91-step=399-consumed_samples=1596.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 641: val_loss reached 5.78398 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.78-step=641-consumed_samples=2564.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:27:37 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.91-step=499-consumed_samples=1996.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 741: val_loss reached 5.62877 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.63-step=741-consumed_samples=2964.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:27:58 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.78-step=641-consumed_samples=2564.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 841: val_loss reached 5.43558 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.44-step=841-consumed_samples=3364.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:28:22 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.63-step=741-consumed_samples=2964.0-last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 941: val_loss reached 5.20438 (best 3.24958), saving model to \"/prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.20-step=941-consumed_samples=3764.0.ckpt\" as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:28:45 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.44-step=841-consumed_samples=3364.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-12-09 08:28:57 nlp_overrides:129] Removing checkpoint: /prompt-tuning/NeMo/examples/nlp/language_modeling/nemo_experiments/megatron_gpt/checkpoints/megatron_gpt--val_loss=5.20-step=941-consumed_samples=3764.0-last.ckpt\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
